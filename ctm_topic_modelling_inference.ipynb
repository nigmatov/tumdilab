{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "init_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.24.2 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"ctm/ctm_training_X_contextual\", \"rb\") as ctm_training_data:\n",
    "    X_contextual = pickle.load(ctm_training_data)\n",
    "\n",
    "with open(\"ctm/ctm_training_X_bow\", \"rb\") as ctm_training_data:\n",
    "    X_bow = pickle.load(ctm_training_data)\n",
    "\n",
    "with open(\"ctm/ctm_training_vectorizer\", \"rb\") as ctm_training_data:\n",
    "    vectorizer = pickle.load(ctm_training_data)\n",
    "    \n",
    "with open(\"ctm/ctm_training_idx2token\", \"rb\") as ctm_training_data:\n",
    "    idx2token = pickle.load(ctm_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4u4Sd9Lw1u54"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file\n",
    "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
    "from contextualized_topic_models.models.ctm import CombinedTM\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "id": "g3JJKYjx2L4q",
    "outputId": "6e86d418-d0d8-48d8-8537-8e9bcb2e3ca5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.sparse\n",
    "import warnings\n",
    "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def get_bag_of_words(data, min_length):\n",
    "    \"\"\"\n",
    "    Creates the bag of words\n",
    "    \"\"\"\n",
    "    vect = [np.bincount(x[x != np.array(None)].astype('int'), minlength=min_length)\n",
    "            for x in data if np.sum(x[x != np.array(None)]) != 0]\n",
    "\n",
    "    vect = scipy.sparse.csr_matrix(vect)\n",
    "    return vect\n",
    "\n",
    "\n",
    "def bert_embeddings_from_file(text_file, sbert_model_to_load, batch_size=200):\n",
    "    \"\"\"\n",
    "    Creates SBERT Embeddings from an input file\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(sbert_model_to_load)\n",
    "    with open(text_file, encoding=\"utf-8\") as filino:\n",
    "        train_text = list(map(lambda x: x, filino.readlines()))\n",
    "\n",
    "    return np.array(model.encode(train_text, show_progress_bar=True, batch_size=batch_size))\n",
    "\n",
    "\n",
    "def bert_embeddings_from_list(texts, sbert_model_to_load, batch_size=200):\n",
    "    \"\"\"\n",
    "    Creates SBERT Embeddings from a list\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(sbert_model_to_load)\n",
    "    return np.array(model.encode(texts, show_progress_bar=True, batch_size=batch_size))\n",
    "\n",
    "\n",
    "class MyTopicModelDataPreparation:\n",
    "\n",
    "    def __init__(self, contextualized_model=None):\n",
    "        self.contextualized_model = contextualized_model\n",
    "        self.vocab = []\n",
    "        self.id2token = {}\n",
    "        self.vectorizer = None\n",
    "\n",
    "    def load(self, contextualized_embeddings, bow_embeddings, id2token, vectorizer):\n",
    "        self.id2token = id2token\n",
    "        self.vectorizer = vectorizer\n",
    "        return CTMDataset(contextualized_embeddings, bow_embeddings, id2token)\n",
    "\n",
    "    def fit(self, text_for_contextual, text_for_bow):\n",
    "        \"\"\"\n",
    "        This method fits the vectorizer and gets the embeddings from the contextual model\n",
    "        \"\"\"\n",
    "\n",
    "        if self.contextualized_model is None:\n",
    "            raise Exception(\"You should define a contextualized model if you want to create the embeddings\")\n",
    "\n",
    "        # TODO: this count vectorizer removes tokens that have len = 1, might be unexpected for the users\n",
    "        self.vectorizer = CountVectorizer()\n",
    "\n",
    "        train_bow_embeddings = self.vectorizer.fit_transform(text_for_bow)\n",
    "        train_contextualized_embeddings = bert_embeddings_from_list(text_for_contextual, self.contextualized_model)\n",
    "        self.vocab = self.vectorizer.get_feature_names()\n",
    "        self.id2token = {k: v for k, v in zip(range(0, len(self.vocab)), self.vocab)}\n",
    "\n",
    "        return CTMDataset(train_contextualized_embeddings, train_bow_embeddings, self.id2token)\n",
    "\n",
    "    def transform(self, text_for_contextual, text_for_bow=None):\n",
    "        \"\"\"\n",
    "        This methods create the input for the prediction. Essentially, it creates the embeddings with the contextualized\n",
    "        model of choice and with trained vectorizer.\n",
    "\n",
    "        If text_for_bow is missing, it should be because we are using ZeroShotTM\n",
    "        \"\"\"\n",
    "\n",
    "        if self.contextualized_model is None:\n",
    "            raise Exception(\"You should define a contextualized model if you want to create the embeddings\")\n",
    "\n",
    "        if text_for_bow is not None:\n",
    "            test_bow_embeddings = self.vectorizer.transform(text_for_bow)\n",
    "        else:\n",
    "            # dummy matrix\n",
    "            warnings.simplefilter('always', DeprecationWarning)\n",
    "            warnings.warn(\"The method did not have in input the text_for_bow parameter. This IS EXPECTED if you \"\n",
    "                          \"are using ZeroShotTM in a cross-lingual setting\")\n",
    "\n",
    "            test_bow_embeddings = scipy.sparse.csr_matrix(np.zeros((len(text_for_contextual), 1)))\n",
    "        test_contextualized_embeddings = bert_embeddings_from_list(text_for_contextual, self.contextualized_model)\n",
    "\n",
    "        return CTMDataset(test_contextualized_embeddings, test_bow_embeddings, self.id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = MyTopicModelDataPreparation(\"stsb-distilroberta-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_dataset = qt.load(contextualized_embeddings = X_contextual, bow_embeddings=X_bow, id2token=idx2token, vectorizer=vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import wordcloud\n",
    "from scipy.special import softmax\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from contextualized_topic_models.utils.early_stopping.early_stopping import EarlyStopping\n",
    "from contextualized_topic_models.networks.decoding_network import DecoderNetwork\n",
    "\n",
    "\n",
    "class MyCTM:\n",
    "    \"\"\"Class to train the contextualized topic model. This is the more general class that we are keeping to\n",
    "    avoid braking code, users should use the two subclasses ZeroShotTM and CombinedTm to do topic modeling.\n",
    "\n",
    "    :param bow_size: int, dimension of input\n",
    "    :param contextual_size: int, dimension of input that comes from BERT embeddings\n",
    "    :param inference_type: string, you can choose between the contextual model and the combined model\n",
    "    :param n_components: int, number of topic components, (default 10)\n",
    "    :param model_type: string, 'prodLDA' or 'LDA' (default 'prodLDA')\n",
    "    :param hidden_sizes: tuple, length = n_layers, (default (100, 100))\n",
    "    :param activation: string, 'softplus', 'relu', (default 'softplus')\n",
    "    :param dropout: float, dropout to use (default 0.2)\n",
    "    :param learn_priors: bool, make priors a learnable parameter (default True)\n",
    "    :param batch_size: int, size of batch to use for training (default 64)\n",
    "    :param lr: float, learning rate to use for training (default 2e-3)\n",
    "    :param momentum: float, momentum to use for training (default 0.99)\n",
    "    :param solver: string, optimizer 'adam' or 'sgd' (default 'adam')\n",
    "    :param num_epochs: int, number of epochs to train for, (default 100)\n",
    "    :param reduce_on_plateau: bool, reduce learning rate by 10x on plateau of 10 epochs (default False)\n",
    "    :param num_data_loader_workers: int, number of data loader workers (default cpu_count). set it to 0 if you are using Windows\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bow_size, contextual_size, inference_type=\"combined\", n_components=10, model_type='prodLDA',\n",
    "                 hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
    "                 learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
    "                 solver='adam', num_epochs=100, reduce_on_plateau=False, num_data_loader_workers=mp.cpu_count()):\n",
    "\n",
    "        if self.__class__.__name__ == \"CTM\":\n",
    "            raise Exception(\"You cannot call this class. Use ZeroShotTM or CombinedTM\")\n",
    "\n",
    "        assert isinstance(bow_size, int) and bow_size > 0, \\\n",
    "            \"input_size must by type int > 0.\"\n",
    "        assert isinstance(n_components, int) and bow_size > 0, \\\n",
    "            \"n_components must by type int > 0.\"\n",
    "        assert model_type in ['LDA', 'prodLDA'], \\\n",
    "            \"model must be 'LDA' or 'prodLDA'.\"\n",
    "        assert isinstance(hidden_sizes, tuple), \\\n",
    "            \"hidden_sizes must be type tuple.\"\n",
    "        assert activation in ['softplus', 'relu'], \\\n",
    "            \"activation must be 'softplus' or 'relu'.\"\n",
    "        assert dropout >= 0, \"dropout must be >= 0.\"\n",
    "        assert isinstance(learn_priors, bool), \"learn_priors must be boolean.\"\n",
    "        assert isinstance(batch_size, int) and batch_size > 0, \\\n",
    "            \"batch_size must be int > 0.\"\n",
    "        assert lr > 0, \"lr must be > 0.\"\n",
    "        assert isinstance(momentum, float) and 0 < momentum <= 1, \\\n",
    "            \"momentum must be 0 < float <= 1.\"\n",
    "        assert solver in ['adam', 'sgd'], \"solver must be 'adam' or 'sgd'.\"\n",
    "        assert isinstance(reduce_on_plateau, bool), \\\n",
    "            \"reduce_on_plateau must be type bool.\"\n",
    "        assert isinstance(num_data_loader_workers, int) and num_data_loader_workers >= 0, \\\n",
    "            \"num_data_loader_workers must by type int >= 0. set 0 if you are using windows\"\n",
    "\n",
    "        self.bow_size = bow_size\n",
    "        self.n_components = n_components\n",
    "        self.model_type = model_type\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.learn_priors = learn_priors\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.contextual_size = contextual_size\n",
    "        self.momentum = momentum\n",
    "        self.solver = solver\n",
    "        self.num_epochs = num_epochs\n",
    "        self.reduce_on_plateau = reduce_on_plateau\n",
    "        self.num_data_loader_workers = num_data_loader_workers\n",
    "\n",
    "        self.model = DecoderNetwork(\n",
    "            bow_size, self.contextual_size, inference_type, n_components, model_type, hidden_sizes, activation,\n",
    "            dropout, learn_priors)\n",
    "        self.early_stopping = None\n",
    "\n",
    "        # init optimizer\n",
    "        if self.solver == 'adam':\n",
    "            self.optimizer = optim.Adam(\n",
    "                self.model.parameters(), lr=lr, betas=(self.momentum, 0.99))\n",
    "        elif self.solver == 'sgd':\n",
    "            self.optimizer = optim.SGD(\n",
    "                self.model.parameters(), lr=lr, momentum=self.momentum)\n",
    "\n",
    "        # init lr scheduler\n",
    "        if self.reduce_on_plateau:\n",
    "            self.scheduler = ReduceLROnPlateau(self.optimizer, patience=10)\n",
    "\n",
    "        # performance attributes\n",
    "        self.best_loss_train = float('inf')\n",
    "\n",
    "        # training attributes\n",
    "        self.model_dir = None\n",
    "        self.train_data = None\n",
    "        self.nn_epoch = None\n",
    "\n",
    "        # validation attributes\n",
    "        self.validation_data = None\n",
    "\n",
    "        # learned topics\n",
    "        self.best_components = None\n",
    "\n",
    "        # Use cuda if available\n",
    "        if torch.cuda.is_available():\n",
    "            self.USE_CUDA = True\n",
    "        else:\n",
    "            self.USE_CUDA = False\n",
    "\n",
    "        if self.USE_CUDA:\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "    def _loss(self, inputs, word_dists, prior_mean, prior_variance,\n",
    "              posterior_mean, posterior_variance, posterior_log_variance):\n",
    "\n",
    "        # KL term\n",
    "        # var division term\n",
    "        var_division = torch.sum(posterior_variance / prior_variance, dim=1)\n",
    "        # diff means term\n",
    "        diff_means = prior_mean - posterior_mean\n",
    "        diff_term = torch.sum(\n",
    "            (diff_means * diff_means) / prior_variance, dim=1)\n",
    "        # logvar det division term\n",
    "        logvar_det_division = \\\n",
    "            prior_variance.log().sum() - posterior_log_variance.sum(dim=1)\n",
    "        # combine terms\n",
    "        KL = 0.5 * (\n",
    "            var_division + diff_term - self.n_components + logvar_det_division)\n",
    "\n",
    "        # Reconstruction term\n",
    "        RL = -torch.sum(inputs * torch.log(word_dists + 1e-10), dim=1)\n",
    "\n",
    "        loss = KL + RL\n",
    "\n",
    "        return loss.sum()\n",
    "\n",
    "    def _train_epoch(self, loader):\n",
    "        \"\"\"Train epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        samples_processed = 0\n",
    "\n",
    "        for batch_samples in loader:\n",
    "            # batch_size x vocab_size\n",
    "            X_bow = batch_samples['X_bow']\n",
    "            X_bow = X_bow.reshape(X_bow.shape[0], -1)\n",
    "            X_contextual = batch_samples['X_contextual']\n",
    "            if self.USE_CUDA:\n",
    "                X_bow = X_bow.cuda()\n",
    "                X_contextual = X_contextual.cuda()\n",
    "\n",
    "            # forward pass\n",
    "            self.model.zero_grad()\n",
    "            prior_mean, prior_variance, posterior_mean, posterior_variance, posterior_log_variance, word_dists =\\\n",
    "                self.model(X_bow, X_contextual)\n",
    "\n",
    "            # backward pass\n",
    "            loss = self._loss(\n",
    "                X_bow, word_dists, prior_mean, prior_variance,\n",
    "                posterior_mean, posterior_variance, posterior_log_variance)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # compute train loss\n",
    "            samples_processed += X_bow.size()[0]\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= samples_processed\n",
    "\n",
    "        return samples_processed, train_loss\n",
    "\n",
    "    def fit(self, train_dataset, validation_dataset=None, save_dir=None, verbose=False, patience=5, delta=0):\n",
    "        \"\"\"\n",
    "        Train the CTM model.\n",
    "\n",
    "        :param train_dataset: PyTorch Dataset class for training data.\n",
    "        :param validation_dataset: PyTorch Dataset class for validation data. If not None, the training stops if validation loss doesn't improve after a given patience\n",
    "        :param save_dir: directory to save checkpoint models to.\n",
    "        :param verbose: verbose\n",
    "        :param patience: How long to wait after last time validation loss improved. Default: 5\n",
    "        :param delta: Minimum change in the monitored quantity to qualify as an improvement. Default: 0\n",
    "\n",
    "        \"\"\"\n",
    "        # Print settings to output file\n",
    "        if verbose:\n",
    "            print(\"Settings: \\n\\\n",
    "                   N Components: {}\\n\\\n",
    "                   Topic Prior Mean: {}\\n\\\n",
    "                   Topic Prior Variance: {}\\n\\\n",
    "                   Model Type: {}\\n\\\n",
    "                   Hidden Sizes: {}\\n\\\n",
    "                   Activation: {}\\n\\\n",
    "                   Dropout: {}\\n\\\n",
    "                   Learn Priors: {}\\n\\\n",
    "                   Learning Rate: {}\\n\\\n",
    "                   Momentum: {}\\n\\\n",
    "                   Reduce On Plateau: {}\\n\\\n",
    "                   Save Dir: {}\".format(\n",
    "                self.n_components, 0.0,\n",
    "                1. - (1. / self.n_components), self.model_type,\n",
    "                self.hidden_sizes, self.activation, self.dropout, self.learn_priors,\n",
    "                self.lr, self.momentum, self.reduce_on_plateau, save_dir))\n",
    "\n",
    "        self.model_dir = save_dir\n",
    "        self.train_data = train_dataset\n",
    "        self.validation_data = validation_dataset\n",
    "        if self.validation_data is not None:\n",
    "            self.early_stopping = EarlyStopping(patience=patience, verbose=verbose, path=save_dir, delta=delta)\n",
    "        train_loader = DataLoader(\n",
    "            self.train_data, batch_size=self.batch_size, shuffle=True,\n",
    "            num_workers=self.num_data_loader_workers)\n",
    "\n",
    "        # init training variables\n",
    "        train_loss = 0\n",
    "        samples_processed = 0\n",
    "\n",
    "        # train loop\n",
    "        pbar = tqdm(self.num_epochs, position=0, leave=True)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.nn_epoch = epoch\n",
    "            # train epoch\n",
    "            s = datetime.datetime.now()\n",
    "            sp, train_loss = self._train_epoch(train_loader)\n",
    "            samples_processed += sp\n",
    "            e = datetime.datetime.now()\n",
    "            pbar.update(1)\n",
    "\n",
    "            if self.validation_data is not None:\n",
    "                validation_loader = DataLoader(self.validation_data, batch_size=self.batch_size, shuffle=True,\n",
    "                                               num_workers=self.num_data_loader_workers)\n",
    "                # train epoch\n",
    "                s = datetime.datetime.now()\n",
    "                val_samples_processed, val_loss = self._validation(validation_loader)\n",
    "                e = datetime.datetime.now()\n",
    "\n",
    "                # report\n",
    "                if verbose:\n",
    "                    print(\"Epoch: [{}/{}]\\tSamples: [{}/{}]\\tValidation Loss: {}\\tTime: {}\".format(\n",
    "                        epoch + 1, self.num_epochs, val_samples_processed,\n",
    "                        len(self.validation_data) * self.num_epochs, val_loss, e - s))\n",
    "\n",
    "                pbar.set_description(\"Epoch: [{}/{}]\\t Seen Samples: [{}/{}]\\tTrain Loss: {}\\tValid Loss: {}\\tTime: {}\".format(\n",
    "                    epoch + 1, self.num_epochs, samples_processed,\n",
    "                    len(self.train_data) * self.num_epochs, train_loss, val_loss, e - s))\n",
    "\n",
    "                self.early_stopping(val_loss, self)\n",
    "                if self.early_stopping.early_stop:\n",
    "                    print(\"Early stopping\")\n",
    "\n",
    "                    break\n",
    "            else:\n",
    "                # save last epoch\n",
    "                self.best_components = self.model.beta\n",
    "                if save_dir is not None:\n",
    "                    self.save(save_dir)\n",
    "            pbar.set_description(\"Epoch: [{}/{}]\\t Seen Samples: [{}/{}]\\tTrain Loss: {}\\tTime: {}\".format(\n",
    "                epoch + 1, self.num_epochs, samples_processed,\n",
    "                len(self.train_data) * self.num_epochs, train_loss, e - s))\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "    def _validation(self, loader):\n",
    "        \"\"\"Validation epoch.\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        samples_processed = 0\n",
    "        for batch_samples in loader:\n",
    "            # batch_size x vocab_size\n",
    "            X_bow = batch_samples['X_bow']\n",
    "            X_bow = X_bow.reshape(X_bow.shape[0], -1)\n",
    "            X_contextual = batch_samples['X_contextual']\n",
    "\n",
    "            if self.USE_CUDA:\n",
    "                X_bow = X_bow.cuda()\n",
    "                X_contextual = X_contextual.cuda()\n",
    "\n",
    "            # forward pass\n",
    "            self.model.zero_grad()\n",
    "            prior_mean, prior_variance, posterior_mean, posterior_variance, posterior_log_variance, word_dists =\\\n",
    "                self.model(X_bow, X_contextual)\n",
    "            loss = self._loss(X_bow, word_dists, prior_mean, prior_variance,\n",
    "                              posterior_mean, posterior_variance, posterior_log_variance)\n",
    "\n",
    "            # compute train loss\n",
    "            samples_processed += X_bow.size()[0]\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss /= samples_processed\n",
    "\n",
    "        return samples_processed, val_loss\n",
    "\n",
    "    def get_thetas(self, dataset, n_samples=20):\n",
    "        \"\"\"\n",
    "        Get the document-topic distribution for a dataset of topics. Includes multiple sampling to reduce variation via\n",
    "        the parameter n_sample.\n",
    "\n",
    "        :param dataset: a PyTorch Dataset containing the documents\n",
    "        :param n_samples: the number of sample to collect to estimate the final distribution (the more the better).\n",
    "        \"\"\"\n",
    "        return self.get_doc_topic_distribution(dataset, n_samples=n_samples)\n",
    "\n",
    "    def get_doc_topic_distribution(self, dataset, n_samples=20):\n",
    "        \"\"\"\n",
    "        Get the document-topic distribution for a dataset of topics. Includes multiple sampling to reduce variation via\n",
    "        the parameter n_sample.\n",
    "\n",
    "        :param dataset: a PyTorch Dataset containing the documents\n",
    "        :param n_samples: the number of sample to collect to estimate the final distribution (the more the better).\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=False,\n",
    "            num_workers=self.num_data_loader_workers)\n",
    "        pbar = tqdm(n_samples, position=0, leave=True)\n",
    "        final_thetas = []\n",
    "        for sample_index in range(n_samples):\n",
    "            with torch.no_grad():\n",
    "                collect_theta = []\n",
    "\n",
    "                for batch_samples in loader:\n",
    "                    # batch_size x vocab_size\n",
    "                    X_bow = batch_samples['X_bow']\n",
    "                    X_bow = X_bow.reshape(X_bow.shape[0], -1)\n",
    "                    X_contextual = batch_samples['X_contextual']\n",
    "\n",
    "                    if self.USE_CUDA:\n",
    "                        X_bow = X_bow.cuda()\n",
    "                        X_contextual = X_contextual.cuda()\n",
    "\n",
    "                    # forward pass\n",
    "                    self.model.zero_grad()\n",
    "                    collect_theta.extend(self.model.get_theta(X_bow, X_contextual).cpu().numpy().tolist())\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(\"Sampling: [{}/{}]\".format(sample_index + 1, n_samples))\n",
    "\n",
    "                final_thetas.append(np.array(collect_theta))\n",
    "        pbar.close()\n",
    "        return np.sum(final_thetas, axis=0) / n_samples\n",
    "\n",
    "    def get_most_likely_topic(self, doc_topic_distribution):\n",
    "        \"\"\" get the most likely topic for each document\n",
    "\n",
    "        :param doc_topic_distribution: ndarray representing the topic distribution of each document\n",
    "        \"\"\"\n",
    "        return np.argmax(doc_topic_distribution, axis=0)\n",
    "\n",
    "\n",
    "    def get_topics(self, k=10):\n",
    "        \"\"\"\n",
    "        Retrieve topic words.\n",
    "\n",
    "        :param k: int, number of words to return per topic, default 10.\n",
    "        \"\"\"\n",
    "        assert k <= self.bow_size, \"k must be <= input size.\"\n",
    "        component_dists = self.best_components\n",
    "        topics = defaultdict(list)\n",
    "        for i in range(self.n_components):\n",
    "            _, idxs = torch.topk(component_dists[i], k)\n",
    "            component_words = [self.train_data.idx2token[idx]\n",
    "                               for idx in idxs.cpu().numpy()]\n",
    "            topics[i] = component_words\n",
    "        return topics\n",
    "\n",
    "    def get_topic_lists(self, k=10):\n",
    "        \"\"\"\n",
    "        Retrieve the lists of topic words.\n",
    "\n",
    "        :param k: (int) number of words to return per topic, default 10.\n",
    "        \"\"\"\n",
    "        assert k <= self.bow_size, \"k must be <= input size.\"\n",
    "        # TODO: collapse this method with the one that just returns the topics\n",
    "        component_dists = self.best_components\n",
    "        topics = []\n",
    "        for i in range(self.n_components):\n",
    "            _, idxs = torch.topk(component_dists[i], k)\n",
    "            component_words = [self.train_data.idx2token[idx]\n",
    "                               for idx in idxs.cpu().numpy()]\n",
    "            topics.append(component_words)\n",
    "        return topics\n",
    "\n",
    "    def _format_file(self):\n",
    "        model_dir = \"contextualized_topic_model_nc_{}_tpm_{}_tpv_{}_hs_{}_ac_{}_do_{}_lr_{}_mo_{}_rp_{}\". \\\n",
    "            format(self.n_components, 0.0, 1 - (1. / self.n_components),\n",
    "                   self.model_type, self.hidden_sizes, self.activation,\n",
    "                   self.dropout, self.lr, self.momentum,\n",
    "                   self.reduce_on_plateau)\n",
    "        return model_dir\n",
    "\n",
    "    def save(self, models_dir=None):\n",
    "        \"\"\"\n",
    "        Save model. (Experimental Feature, not tested)\n",
    "\n",
    "        :param models_dir: path to directory for saving NN models.\n",
    "        \"\"\"\n",
    "        warnings.simplefilter('always', Warning)\n",
    "        warnings.warn(\"This is an experimental feature that we has not been fully tested. Refer to the following issue:\"\n",
    "                      \"https://github.com/MilaNLProc/contextualized-topic-models/issues/38\",\n",
    "                      Warning)\n",
    "\n",
    "        if (self.model is not None) and (models_dir is not None):\n",
    "\n",
    "            model_dir = self._format_file()\n",
    "            if not os.path.isdir(os.path.join(models_dir, model_dir)):\n",
    "                os.makedirs(os.path.join(models_dir, model_dir))\n",
    "\n",
    "            filename = \"epoch_{}\".format(self.nn_epoch) + '.pth'\n",
    "            fileloc = os.path.join(models_dir, model_dir, filename)\n",
    "            with open(fileloc, 'wb') as file:\n",
    "                torch.save({'state_dict': self.model.state_dict(),\n",
    "                            'dcue_dict': self.__dict__}, file)\n",
    "\n",
    "    def load(self, model_dir, epoch):\n",
    "        \"\"\"\n",
    "        Load a previously trained model. (Experimental Feature, not tested)\n",
    "\n",
    "        :param model_dir: directory where models are saved.\n",
    "        :param epoch: epoch of model to load.\n",
    "        \"\"\"\n",
    "\n",
    "        warnings.simplefilter('always', Warning)\n",
    "        warnings.warn(\"This is an experimental feature that we has not been fully tested. Refer to the following issue:\"\n",
    "                      \"https://github.com/MilaNLProc/contextualized-topic-models/issues/38\",\n",
    "                      Warning)\n",
    "\n",
    "        epoch_file = \"epoch_\" + str(epoch) + \".pth\"\n",
    "        model_file = os.path.join(model_dir, epoch_file)\n",
    "        with open(model_file, 'rb') as model_dict:\n",
    "            checkpoint = torch.load(model_dict, map_location={'cuda:0': 'cpu'})\n",
    "\n",
    "        for (k, v) in checkpoint['dcue_dict'].items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    def get_topic_word_matrix(self):\n",
    "        \"\"\"\n",
    "        Return the topic-word matrix (dimensions: number of topics x length of the vocabulary).\n",
    "        If model_type is LDA, the matrix is normalized; otherwise the matrix is unnormalized.\n",
    "        \"\"\"\n",
    "        return self.model.topic_word_matrix.cpu().detach().numpy()\n",
    "\n",
    "    def get_topic_word_distribution(self):\n",
    "        \"\"\"\n",
    "        Return the topic-word distribution (dimensions: number of topics x length of the vocabulary).\n",
    "        \"\"\"\n",
    "        mat = self.get_topic_word_matrix()\n",
    "        return softmax(mat, axis=1)\n",
    "\n",
    "    def get_word_distribution_by_topic_id(self, topic):\n",
    "        \"\"\"\n",
    "        Return the word probability distribution of a topic sorted by probability.\n",
    "\n",
    "        :param topic: id of the topic (int)\n",
    "\n",
    "        :returns list of tuples (word, probability) sorted by the probability in descending order\n",
    "        \"\"\"\n",
    "        if topic >= self.n_components:\n",
    "            raise Exception('Topic id must be lower than the number of topics')\n",
    "        else:\n",
    "            wd = self.get_topic_word_distribution()\n",
    "            t = [(word, wd[topic][idx]) for idx, word in self.train_data.idx2token.items()]\n",
    "            t = sorted(t, key=lambda x: -x[1])\n",
    "        return t\n",
    "\n",
    "    def get_wordcloud(self, topic_id, n_words=5, background_color=\"black\", width=1000, height=400):\n",
    "        \"\"\"\n",
    "        Plotting the wordcloud. It is an adapted version of the code found here:\n",
    "        http://amueller.github.io/word_cloud/auto_examples/simple.html#sphx-glr-auto-examples-simple-py and\n",
    "        here https://github.com/ddangelov/Top2Vec/blob/master/top2vec/Top2Vec.py\n",
    "\n",
    "        :param topic_id: id of the topic\n",
    "        :param n_words: number of words to show in word cloud\n",
    "        :param background_color: color of the background\n",
    "        :param width: width of the produced image\n",
    "        :param height: height of the produced image\n",
    "        \"\"\"\n",
    "        word_score_list = self.get_word_distribution_by_topic_id(topic_id)[:n_words]\n",
    "        word_score_dict = {tup[0]: tup[1] for tup in word_score_list}\n",
    "        plt.figure(figsize=(10, 4), dpi=200)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(wordcloud.WordCloud(width=width, height=height, background_color=background_color\n",
    "                                       ).generate_from_frequencies(word_score_dict))\n",
    "        plt.title(\"Displaying Topic \" + str(topic_id), loc='center', fontsize=24)\n",
    "        plt.show()\n",
    "\n",
    "    def get_predicted_topics(self, dataset, n_samples):\n",
    "        \"\"\"\n",
    "        Return the a list containing the predicted topic for each document (length: number of documents).\n",
    "\n",
    "        :param dataset: CTMDataset to infer topics\n",
    "        :param n_samples: number of sampling of theta\n",
    "        :return: the predicted topics\n",
    "        \"\"\"\n",
    "        predicted_topics = []\n",
    "        thetas = self.get_doc_topic_distribution(dataset, n_samples)\n",
    "\n",
    "        for idd in range(len(dataset)):\n",
    "            predicted_topic = np.argmax(thetas[idd] / np.sum(thetas[idd]))\n",
    "            predicted_topics.append(predicted_topic)\n",
    "        return predicted_topics\n",
    "\n",
    "    def get_ldavis_data_format(self, vocab, dataset, n_samples):\n",
    "        \"\"\"\n",
    "        Returns the data that can be used in input to pyldavis to plot\n",
    "        the topics\n",
    "        \"\"\"\n",
    "        term_frequency = dataset.X_bow.toarray().sum(axis=0)\n",
    "        doc_lengths = dataset.X_bow.toarray().sum(axis=1)\n",
    "        term_topic = self.get_topic_word_distribution()\n",
    "        doc_topic_distribution = self.get_doc_topic_distribution(dataset, n_samples=n_samples)\n",
    "\n",
    "        data = {'topic_term_dists': term_topic,\n",
    "                'doc_topic_dists': doc_topic_distribution,\n",
    "                'doc_lengths': doc_lengths,\n",
    "                'vocab': vocab,\n",
    "                'term_frequency': term_frequency}\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class ZeroShotTM(MyCTM):\n",
    "    \"\"\"ZeroShotTM, as described in https://arxiv.org/pdf/2004.07737v1.pdf\n",
    "\n",
    "    :param bow_size: int, dimension of input\n",
    "    :param contextual_size: int, dimension of input that comes from BERT embeddings\n",
    "    :param n_components: int, number of topic components, (default 10)\n",
    "    :param model_type: string, 'prodLDA' or 'LDA' (default 'prodLDA')\n",
    "    :param hidden_sizes: tuple, length = n_layers, (default (100, 100))\n",
    "    :param activation: string, 'softplus', 'relu', (default 'softplus')\n",
    "    :param dropout: float, dropout to use (default 0.2)\n",
    "    :param learn_priors: bool, make priors a learnable parameter (default True)\n",
    "    :param batch_size: int, size of batch to use for training (default 64)\n",
    "    :param lr: float, learning rate to use for training (default 2e-3)\n",
    "    :param momentum: float, momentum to use for training (default 0.99)\n",
    "    :param solver: string, optimizer 'adam' or 'sgd' (default 'adam')\n",
    "    :param num_epochs: int, number of epochs to train for, (default 100)\n",
    "    :param reduce_on_plateau: bool, reduce learning rate by 10x on plateau of 10 epochs (default False)\n",
    "    :param num_data_loader_workers: int, number of data loader workers (default cpu_count). set it to 0 if you are using Windows\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bow_size, contextual_size, n_components=10, model_type='prodLDA',\n",
    "                 hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
    "                 learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
    "                 solver='adam', num_epochs=100, reduce_on_plateau=False, num_data_loader_workers=mp.cpu_count()):\n",
    "        inference_type = \"zeroshot\"\n",
    "        super().__init__(bow_size, contextual_size, inference_type, n_components, model_type,\n",
    "                         hidden_sizes, activation, dropout,\n",
    "                         learn_priors, batch_size, lr, momentum,\n",
    "                         solver, num_epochs, reduce_on_plateau, num_data_loader_workers)\n",
    "\n",
    "\n",
    "class MyCombinedTM(MyCTM):\n",
    "    \"\"\"CombinedTM, as described in https://arxiv.org/pdf/2004.03974.pdf\n",
    "\n",
    "    :param bow_size: int, dimension of input\n",
    "    :param contextual_size: int, dimension of input that comes from BERT embeddings\n",
    "    :param n_components: int, number of topic components, (default 10)\n",
    "    :param model_type: string, 'prodLDA' or 'LDA' (default 'prodLDA')\n",
    "    :param hidden_sizes: tuple, length = n_layers, (default (100, 100))\n",
    "    :param activation: string, 'softplus', 'relu', (default 'softplus')\n",
    "    :param dropout: float, dropout to use (default 0.2)\n",
    "    :param learn_priors: bool, make priors a learnable parameter (default True)\n",
    "    :param batch_size: int, size of batch to use for training (default 64)\n",
    "    :param lr: float, learning rate to use for training (default 2e-3)\n",
    "    :param momentum: float, momentum to use for training (default 0.99)\n",
    "    :param solver: string, optimizer 'adam' or 'sgd' (default 'adam')\n",
    "    :param num_epochs: int, number of epochs to train for, (default 100)\n",
    "    :param reduce_on_plateau: bool, reduce learning rate by 10x on plateau of 10 epochs (default False)\n",
    "    :param num_data_loader_workers: int, number of data loader workers (default cpu_count). set it to 0 if you are using Windows\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bow_size, contextual_size, n_components=10, model_type='prodLDA',\n",
    "                 hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
    "                 learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
    "                 solver='adam', num_epochs=100, reduce_on_plateau=False, num_data_loader_workers=mp.cpu_count()):\n",
    "        inference_type = \"combined\"\n",
    "        super().__init__(bow_size, contextual_size, inference_type, n_components, model_type,\n",
    "                         hidden_sizes, activation, dropout,\n",
    "                         learn_priors, batch_size, lr, momentum,\n",
    "                         solver, num_epochs, reduce_on_plateau, num_data_loader_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3rbBGqV2SzR",
    "outputId": "40c0208a-00c7-4427-b586-80f160573c16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:440: Warning: This is an experimental feature that we has not been fully tested. Refer to the following issue:https://github.com/MilaNLProc/contextualized-topic-models/issues/38\n"
     ]
    }
   ],
   "source": [
    "ctm_10 = MyCombinedTM(bow_size=422939, contextual_size=768, n_components=50, num_epochs=100, batch_size=256,num_data_loader_workers=0)\n",
    "\n",
    "ctm_10.load(\"ctm/10_topics\",epoch=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:440: Warning: This is an experimental feature that we has not been fully tested. Refer to the following issue:https://github.com/MilaNLProc/contextualized-topic-models/issues/38\n"
     ]
    }
   ],
   "source": [
    "ctm_50 = MyCombinedTM(bow_size=422939, contextual_size=768, n_components=50, num_epochs=100, batch_size=256,num_data_loader_workers=0)\n",
    "\n",
    "ctm_50.load(\"ctm/50_topics\",epoch=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FOrDzaERY_-i"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "inference_texts = []\n",
    "\n",
    "# https://www.dw.com/en/cubas-covid-vaccine-rivals-biontech-pfizer-moderna/a-58052365\n",
    "text_for_inference = \"\"\"\n",
    "Cuba's COVID vaccine rivals BioNTech-Pfizer, Moderna Cuba's health authorities said this week the domestically produced Abdala vaccine has proven to be 92% effective against the coronavirus in clinical trials. DW takes a closer look.\n",
    "In a measure of its ambitious efforts to be vaccine self-reliant, Cuba has named one of its homegrown jabs Abdala, after a famous dramatic verse by independence hero and national icon Jose Marti. In the verse, the young hero, Abdala, heads to war to defend his fatherland, full of patriotic fervor no matter how strong and powerful the enemy.\n",
    "\n",
    "From the perspective of many Cubans, it's the perfect name for the first COVID-19 vaccine to be developed in Latin America. And the perfect imagery for the story of a tiny island of 11 million inhabitants eager to show it can't be broken by a deadly virus and a 60-year economic blockade by the United States, and a country that boasts several brilliant scientists of its own.\n",
    "\n",
    "Cuba's new scientist star\n",
    "\n",
    "One of them is Gerardo Enrique Guillen Nieto, director of biomedical research at the Center for Genetic Engineering and Biotechnology (CIGB) in Havana where Abdala was developed.\n",
    "\n",
    "Last Sunday on Father's Day, Cuban television ran a commercial featuring the 58-year-old Guillen Nieto. Accompanied by melodramatic music, it opened with the scientist in his clinic while his son talked off camera about how his father works tirelessly for his family and the people.\n",
    "\n",
    "Guillen Nieto said he gathered researchers from all fields to develop the vaccines\n",
    "\n",
    "\"We have worked full time since the beginning of the pandemic, every Saturday, every Sunday, from early in the morning until late at night, without even a moment's rest,\" the highly respected scientist said in the clip. \"And we are very euphoric because the results have exceeded all our expectations. We knew the vaccine was very good, but not even I expected such a result.\"\n",
    "\n",
    "Charting its own course\n",
    "\n",
    "According to the state-run biotech corporation, BioCubaFarma, Abdala has proven about 92.28% effective against COVID-19 in clinical trials, which would put it the same league as the most effective vaccines BioNTech-Pfizer and Moderna. Huge applause erupted in the auditorium of the CIGB this week when the impressive results were announced.\n",
    "\n",
    "Since then, Guillen Nieto has been inundated with interview requests. The whole world wants to know Abdala's formula for success. The Cuban vaccine is neither a vector vaccine nor does it work with mRNA technology. Instead, it's a so-called protein vaccine. That means it carries a portion of the spike protein that the virus uses to bind to human cells. It docks onto the receptors of the virus' own spike protein, thus triggering an immune reaction. The scientists are using yeast as a receptor-binding domain.\n",
    "\n",
    "The government vaccination program was rolled out in mid-May with Abdala and the second homegrown vaccine, Soberana 2, even before the completion of the third phase of clinical trials. These are the first vaccines on the island since Cuba declined importing any shots from Russia or China. Cuba has also decided against joining the UN-backed COVAX initiative, a global project aimed at getting COVID-19 shots to countries regardless of their wealth.\n",
    "\n",
    "Cuba's cigar-makers optimistic despite COVID\n",
    "\"We know that in the end we always have to rely on ourselves, on our own strengths and abilities,\" said Guillen Nieto, alluding to the political isolation caused by the US embargo. \"The result is a health care system that is not only free of cost but also centrally controlled, and that has perfected the ability to respond quickly to disasters, be it with clinical trials, with vaccination campaigns or even the production of a vaccine.\"\n",
    "\n",
    "Vaccinations to curb rising COVID infections\n",
    "\n",
    "According to Guillen Nieto, 2.2 million Cubans have already received their first vaccination, 1.7 million their second and 900,000 the third dose.\n",
    "\n",
    "Abdala is administered in three doses, with two weeks between each vaccination. Based on the government's ambitious plans, 70% of country's population should receive their shots by August.\n",
    "\n",
    "It's a race against time because the number of new infections on the Caribbean island is steadily rising with more than 2,000 cases a day. Nearly 1,200 people have died of COVID-19 in Cuba. Guillen Nieto is counting on the vaccination campaign to give him a decisive advantage over all other countries in the world in the fight against the virus.\n",
    "\n",
    "Cuba rolled out its vaccination campaign in mid-May\n",
    "\n",
    "\"Here there is an unprecedented level of trust in the Cuban health system,\" he said. \"For example, we never have problems finding volunteers when it comes to clinical trials. In Cuba, people are extremely eager to be vaccinated. No one here would think of not getting inoculated because everyone knows how important vaccinations are.\"\n",
    "\n",
    "An independent panel of experts in Havana will now scrutinize the Abdala vaccine, and official emergency approval is expected in the next two weeks. After that, Cuba could also apply to the World Health Organization (WHO) for approval of Abdala for international use. Bolivia, Jamaica, Venezuela, Argentina and Mexico have already signaled interest.\n",
    "\n",
    "WHO shares optimism\n",
    "\n",
    "But is Abdala really the miracle vaccine that the numbers promise? Perhaps Jose Moya is the man best placed to assess this. The Peruvian doctor started out as an epidemiologist 30 years ago in his native Ayacucho, and then worked for Doctors Without Borders in Guatemala, Mozambique and Nigeria.\n",
    "\n",
    "For the past two years, Moya has been the representative in Cuba of PAHO (Pan American Health Organization), a regional organization of the WHO with 27 country offices. And, he trusts the Cuban figures.\n",
    "\n",
    "\"The CIGB Research Institute has 30 years of experience in vaccine research. I trust the results that have been published. These are serious studies, with the participation of researchers and institutions committed to science,\" Moya said.\n",
    "\n",
    "Facing Latin America's worst outbreak\n",
    "The best proof is the fact that 80% of all of Cuba's vaccines are produced in the country itself, Moya said. He was not surprised by the high efficacy of Abdala, saying it was simply the logical consequence of a health care system that had been performing steadily well for decades. \"Already, the results published by the scientists beforehand showed a good response in terms of antibody production,\" he said.\n",
    "\n",
    "Cuban President Miguel DÃ­az-Canel, however, does not want to dwell on scientific assessments of the new vaccine. For him, the country's drive to pursue homegrown solutions rather than importing foreign vaccines are a triumph of Cuba's biotech industry.\n",
    "\n",
    "\"This success can only be compared to the greatness of our sacrifices. It is an example of the pride with which a country treats its pharmaceutical industry, which has been living with the US economic embargo since 1962,\" he said.\n",
    "\n",
    "This article has been translated from German\n",
    "\"\"\"\n",
    "\n",
    "inference_texts.append(text_for_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "import warnings\n",
    "\n",
    "class MyWhiteSpacePreprocessing():\n",
    "    \"\"\"\n",
    "    Provides a very simple preprocessing script that filters infrequent tokens from text\n",
    "    \"\"\"\n",
    "    def __init__(self, documents, stopwords_language=\"english\", vocabulary_size=2000):\n",
    "        \"\"\"\n",
    "        :param documents: list of strings\n",
    "        :param stopwords_language: string of the language of the stopwords (see nltk stopwords)\n",
    "        :param vocabulary_size: the number of most frequent words to include in the documents. Infrequent words will be discarded from the list of preprocessed documents\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.stopwords = set(stop_words.words(stopwords_language))\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        Note that if after filtering some documents do not contain words we remove them. That is why we return also the\n",
    "        list of unpreprocessed documents.\n",
    "        :return: preprocessed documents, unpreprocessed documents and the vocabulary list\n",
    "        \"\"\"\n",
    "        preprocessed_docs_tmp = self.documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
    "                             for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        vectorizer = CountVectorizer(max_features=self.vocabulary_size, token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
    "        vocabulary = set(vectorizer.get_feature_names())\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in vocabulary])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        preprocessed_docs, unpreprocessed_docs, indices_of_empty_docs = [], [], []\n",
    "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
    "            if len(doc) > 0:\n",
    "                preprocessed_docs.append(doc)\n",
    "                unpreprocessed_docs.append(self.documents[i])\n",
    "            else:\n",
    "                indices_of_empty_docs.append(i)\n",
    "                \n",
    "\n",
    "        return preprocessed_docs, unpreprocessed_docs, list(vocabulary), indices_of_empty_docs\n",
    "\n",
    "\n",
    "class MySimplePreprocessing(WhiteSpacePreprocessing):\n",
    "    def __init__(self, documents, stopwords_language=\"english\"):\n",
    "        super().__init__(documents, stopwords_language)\n",
    "        warnings.simplefilter('always', DeprecationWarning)\n",
    "\n",
    "        if self.__class__.__name__ == \"CTM\":\n",
    "\n",
    "            warnings.warn(\"SimplePrepocessing is deprecated and will be removed in version 2.0, \"\n",
    "                          \"use WhiteSpacePreprocessing\", DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sp = MyWhiteSpacePreprocessing(inference_texts, \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessed_documents, test_unpreprocessed_documents, test_vocab, indices_of_empty_docs = test_sp.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178e9149d972409d8dbe8acb3c88bf12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testing_dataset = qt.transform(text_for_contextual=test_preprocessed_documents, text_for_bow=test_unpreprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [5/5]: : 5it [00:00, 13.36it/s]\n",
      "Sampling: [5/5]: : 5it [00:00, 29.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5507471561431885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "ctm_10.USE_CUDA = False\n",
    "ctm_50.USE_CUDA = False\n",
    "topics_predictions_10 = ctm_10.get_doc_topic_distribution(testing_dataset, n_samples=5)\n",
    "topics_predictions_50 = ctm_50.get_doc_topic_distribution(testing_dataset, n_samples=5)# get all the topic predictions\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328.9017505645752\n"
     ]
    }
   ],
   "source": [
    "init_end_time = time.time()\n",
    "print(init_end_time - init_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_predictions_10_list = topics_predictions_10.tolist()\n",
    "topics_predictions_50_list = topics_predictions_50.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_predictions_10_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_predictions_50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_of_empty_doc in indices_of_empty_docs:\n",
    "    topics_predictions_10_list.insert(index_of_empty_doc, [0] * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_of_empty_doc in indices_of_empty_docs:\n",
    "    topics_predictions_50_list.insert(index_of_empty_doc, [0] * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_predictions_10_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_predictions_50_list"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "topic_modelling_elastic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1a9188675c6848c9ac8d80d66783b91d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2cac774d45bc40cdaa83d532044f2d4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2dac04a17e434029a118067b68bbffe6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f0d5e88d0874006a0b407cafd95677d",
      "placeholder": "â",
      "style": "IPY_MODEL_6c2df116ac2c40c1a758b6e6dc24d664",
      "value": " 250/250 [03:59&lt;00:00,  1.14it/s]"
     }
    },
    "36580e6111c2469fb03d1d634332d9be": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "496fc9ba481e40929a653c026315b31d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d8fe7429bfc6416ea5a69ac9f6e4e6b6",
       "IPY_MODEL_b965d54e95db469f8d35e689ce414c77",
       "IPY_MODEL_520120e7fa2241fa9e86d2ce4d682d3f"
      ],
      "layout": "IPY_MODEL_aeebb033f8f045c3b5437e90fc3c2b67"
     }
    },
    "520120e7fa2241fa9e86d2ce4d682d3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36580e6111c2469fb03d1d634332d9be",
      "placeholder": "â",
      "style": "IPY_MODEL_2cac774d45bc40cdaa83d532044f2d4f",
      "value": " 305M/305M [00:37&lt;00:00, 10.7MB/s]"
     }
    },
    "554db03e4dc14e1591c55f31d087d358": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c74a6b07c244ae382417385d2c36818": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_79c2b86d4e0144a9b6e27f03f7c7f51c",
       "IPY_MODEL_d14c44dd694e4a0ba3ab258bf2bf4ae1",
       "IPY_MODEL_2dac04a17e434029a118067b68bbffe6"
      ],
      "layout": "IPY_MODEL_8d16ae3bc3b24cfd8b72994d094981c5"
     }
    },
    "5f0d5e88d0874006a0b407cafd95677d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c2df116ac2c40c1a758b6e6dc24d664": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79c2b86d4e0144a9b6e27f03f7c7f51c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_554db03e4dc14e1591c55f31d087d358",
      "placeholder": "â",
      "style": "IPY_MODEL_1a9188675c6848c9ac8d80d66783b91d",
      "value": "Batches: 100%"
     }
    },
    "8d16ae3bc3b24cfd8b72994d094981c5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aeebb033f8f045c3b5437e90fc3c2b67": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8a1b9a2bf864ac49785929752505db4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b965d54e95db469f8d35e689ce414c77": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba950252e5db4043b17a398c71be2bf4",
      "max": 305208253,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c97225c6ea6d4a09b68e53a1a3518604",
      "value": 305208253
     }
    },
    "ba950252e5db4043b17a398c71be2bf4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c97225c6ea6d4a09b68e53a1a3518604": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d14c44dd694e4a0ba3ab258bf2bf4ae1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e431272fbf07440093836badd9c02dd3",
      "max": 250,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f73831d0ff7146b98c09fdf7e7d98218",
      "value": 250
     }
    },
    "d8fe7429bfc6416ea5a69ac9f6e4e6b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb06d5d1eaa8494f990e40422664ac34",
      "placeholder": "â",
      "style": "IPY_MODEL_b8a1b9a2bf864ac49785929752505db4",
      "value": "100%"
     }
    },
    "e431272fbf07440093836badd9c02dd3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb06d5d1eaa8494f990e40422664ac34": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f73831d0ff7146b98c09fdf7e7d98218": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

# -*- coding: utf-8 -*-
"""embeddings_generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-PCtoDKgQfBINQoErGLwmru7Vr6sRt4s
"""

# !pip install transformers
# !pip install sentence_transformers
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer

import json
import sys
import torch

device = 'cpu'
batch_size = 10

distilbert_tokenizer = AutoTokenizer.from_pretrained("distilbert-base-cased")
# Use DistilBERT-cased version.
distilbert = AutoModel.from_pretrained("distilbert-base-cased")

mpnet = SentenceTransformer('stsb-mpnet-base-v2')
distilroberta = SentenceTransformer('stsb-distilroberta-base-v2')

"""According to the official docs(https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__) we can just provide a sequence or a list of sequences (non-tokenized) without cutting ourselves the first 512 words (BERT doesn't use words per se anyway, it preprocesses them via BPE)."""

def distilbert_encode(document):
    embedding = distilbert(**distilbert_tokenizer(document, return_tensors='pt', truncation=True, padding=True))[0].detach().squeeze()
    return torch.mean(embedding, dim=0).detach().numpy()

with open(sys.argv[1]) as f:
  with open(sys.argv[1] + '_with_embeddings', 'w') as fout:
    docs = []
    datas = []
    count = 0

    for line in f:
        doc = json.loads(line)
        docs.append(doc)
        datas.append(doc['_source']['content_t'])

        count += 1
        if count == batch_size:
            distilbert_embeddings = distilbert_encode(datas)
            mpnet_embeddings = mpnet.encode(datas)
            distilroberta_embeddings = distilroberta.encode(datas)
  
            for idx, doc in enumerate(docs):
                doc['_source']['distilbert_embedding'] = distilbert_embeddings[idx].tolist()
                doc['_source']['mpnet_embedding'] = mpnet_embeddings[idx].tolist()
                doc['_source']['distilroberta_embedding'] = distilroberta_embeddings[idx].tolist()

                json.dump(doc, fout)
            datas = []
            docs = []
            count = 0
